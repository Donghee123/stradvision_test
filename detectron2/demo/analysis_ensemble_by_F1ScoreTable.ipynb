{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.ops \n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as func\n",
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2.evaluation.coco_evaluation import instances_to_coco_json\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config /home/dongheehan/anaconda3/envs/objectdetection/lib/python3.7/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "Model ready\n",
      "Downloading split 'validation' to '/home/dongheehan/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/dongheehan/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'evaluate-ensenble-detections'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Name:        evaluate-ensenble-detections\n",
      "Media type:  image\n",
      "Num samples: 5000\n",
      "Persistent:  True\n",
      "Tags:        ['validation']\n",
      "Sample fields:\n",
      "    id:                fiftyone.core.fields.ObjectIdField\n",
      "    filepath:          fiftyone.core.fields.StringField\n",
      "    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    ground_truth:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    ensemble:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    eval4_tp:          fiftyone.core.fields.IntField\n",
      "    eval4_fp:          fiftyone.core.fields.IntField\n",
      "    eval4_fn:          fiftyone.core.fields.IntField\n",
      "    eval_union_tp:     fiftyone.core.fields.IntField\n",
      "    eval_union_fp:     fiftyone.core.fields.IntField\n",
      "    eval_union_fn:     fiftyone.core.fields.IntField\n",
      "    eval_f1ranking_tp: fiftyone.core.fields.IntField\n",
      "    eval_f1ranking_fp: fiftyone.core.fields.IntField\n",
      "    eval_f1ranking_fn: fiftyone.core.fields.IntField\n",
      "<Detection: {\n",
      "    'id': '62a3fd06fdfbb6089ce458c6',\n",
      "    'attributes': BaseDict({}),\n",
      "    'tags': BaseList([]),\n",
      "    'label': 'potted plant',\n",
      "    'bounding_box': BaseList([\n",
      "        0.37028125,\n",
      "        0.3345305164319249,\n",
      "        0.038593749999999996,\n",
      "        0.16314553990610328,\n",
      "    ]),\n",
      "    'mask': None,\n",
      "    'confidence': None,\n",
      "    'index': None,\n",
      "    'supercategory': 'furniture',\n",
      "    'iscrowd': 0,\n",
      "    'eval4': 'tp',\n",
      "    'eval4_id': '62a4d61f140b464e4fa7fe6a',\n",
      "    'eval4_iou': 0.5737284663218926,\n",
      "    'eval_union': 'fn',\n",
      "    'eval_union_id': '',\n",
      "    'eval_f1ranking': 'fn',\n",
      "    'eval_f1ranking_id': '',\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "PREDICTION_PATH ='./predictions'\n",
    "inferencemode = 'COCO-Detection'\n",
    "expendName = '.yaml'\n",
    "\n",
    "load_model_names = ['retinanet_R_50_FPN_1x', 'retinanet_R_50_FPN_3x', 'retinanet_R_101_FPN_3x']\n",
    "\n",
    "load_model_name = 'ensemble'\n",
    "\n",
    "# load model\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(os.path.join(inferencemode, load_model_names[0] + expendName)))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "#Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(os.path.join(inferencemode, load_model_names[0] + expendName))\n",
    "\n",
    "# load\n",
    "with open(os.path.join(PREDICTION_PATH, f'{load_model_names[0]}_predictions.pickle'), 'rb') as f:\n",
    "    model1_predictions = pickle.load(f)\n",
    "\n",
    "# load\n",
    "with open(os.path.join(PREDICTION_PATH, f'{load_model_names[1]}_predictions.pickle'), 'rb') as f:\n",
    "    model2_predictions = pickle.load(f)\n",
    "\n",
    "# load\n",
    "with open(os.path.join(PREDICTION_PATH, f'{load_model_names[2]}_predictions.pickle'), 'rb') as f:\n",
    "    model3_predictions = pickle.load(f)\n",
    "\n",
    "\n",
    "print(\"Model ready\")\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    dataset_name=\"evaluate-ensenble-detections\",\n",
    ")\n",
    "dataset.persistent = True\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print a ground truth detection\n",
    "sample = dataset.first()\n",
    "print(sample.ground_truth.detections[0])\n",
    "\n",
    "session = fo.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog\n",
    "import pickle\n",
    "\n",
    "# Choose a random subset of 100 samples to add predictions to\n",
    "predictions_view = dataset.take(5000,seed=51) # limit size\n",
    "\n",
    "#Get class list\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "classes = metadata.get(\"thing_classes\", None)\n",
    "\n",
    "overlabthreshold = 0.15\n",
    "predictions= []\n",
    "\n",
    "# load\n",
    "with open(os.path.join('rankingperclasschart.pickle'), 'rb') as f:\n",
    "    rankingperclasschart = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retinanet_R_101_FPN_3x' == rankingperclasschart['person'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleInference(sample ,predictor, modelname, rankingperclasschart, detections):\n",
    "     # Perform inference\n",
    "\n",
    "        image = cv2.imread(sample.filepath)\n",
    "        h, w, c = image.shape\n",
    "\n",
    "        filename = os.path.basename(sample.filepath)\n",
    "        prediction = predictor[filename]\n",
    "\n",
    "        boxes =  prediction[0]\n",
    "        labels = prediction[1]\n",
    "        scores = prediction[2]\n",
    "        tensor_boxes =  torch.tensor(boxes)\n",
    "        tensor_scores = torch.tensor(scores)\n",
    "\n",
    "        results = torchvision.ops.nms(tensor_boxes, tensor_scores, overlabthreshold)\n",
    "\n",
    "        # Convert detections to FiftyOne format\n",
    "        \n",
    "        for nIndex, (label, score, box) in enumerate(zip(labels, scores, boxes)):\n",
    "            # only use best F1 Score model on class\n",
    "            if (nIndex in results) is False or (modelname != rankingperclasschart[classes[label]][0][0]):\n",
    "                continue\n",
    "\n",
    "            #print(f'class {classes[label]} : {modelname}')\n",
    "            \n",
    "            x, y, width, height = box\n",
    "\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x + width\n",
    "            y2 = y + height\n",
    "            \n",
    "            rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
    "\n",
    "            detections.append(\n",
    "                fo.Detection(\n",
    "                    label=classes[label],\n",
    "                    bounding_box=rel_box,\n",
    "                    confidence=score\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fo.ProgressBar() as pb:\n",
    "    for sample in pb(predictions_view):\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(sample.filepath)\n",
    "        h, w, c = image.shape\n",
    "\n",
    "        detections = []\n",
    "\n",
    "        # Perform inference\n",
    "        SingleInference(sample ,model1_predictions, load_model_names[0], rankingperclasschart, detections)\n",
    "        #print('by model1 : ', len(detections))\n",
    "        SingleInference(sample, model2_predictions, load_model_names[1], rankingperclasschart, detections)\n",
    "        #print('by model2 : ', len(detections))\n",
    "        SingleInference(sample, model3_predictions, load_model_names[2], rankingperclasschart, detections)        \n",
    "        #print('by model3 : ', len(detections))\n",
    "\n",
    "        # Save predictions to dataset\n",
    "        sample[load_model_name] = fo.Detections(detections=detections)\n",
    "        sample.save()\n",
    "\n",
    "\n",
    "print(\"Finished adding predictions\")\n",
    "session.view = None\n",
    "session.view = predictions_view\n",
    "session.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiftyone_classes = dataset.default_classes\n",
    "\n",
    "#Get class list\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "COCO_classes = metadata.get(\"thing_classes\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Only contains detections with confidence >= 0.75\n",
    "high_conf_view = dataset.filter_labels(load_model_name, F(\"confidence\") > 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:     evaluate-ensenble-detections\n",
      "Media type:  image\n",
      "Num samples: 4985\n",
      "Tags:        ['validation']\n",
      "Sample fields:\n",
      "    id:                fiftyone.core.fields.ObjectIdField\n",
      "    filepath:          fiftyone.core.fields.StringField\n",
      "    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    ground_truth:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    ensemble:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    eval4_tp:          fiftyone.core.fields.IntField\n",
      "    eval4_fp:          fiftyone.core.fields.IntField\n",
      "    eval4_fn:          fiftyone.core.fields.IntField\n",
      "    eval_union_tp:     fiftyone.core.fields.IntField\n",
      "    eval_union_fp:     fiftyone.core.fields.IntField\n",
      "    eval_union_fn:     fiftyone.core.fields.IntField\n",
      "    eval_f1ranking_tp: fiftyone.core.fields.IntField\n",
      "    eval_f1ranking_fp: fiftyone.core.fields.IntField\n",
      "    eval_f1ranking_fn: fiftyone.core.fields.IntField\n",
      "View stages:\n",
      "    1. FilterLabels(field='ensemble', filter={'$gt': ['$$this.confidence', 0.15]}, only_matches=True, trajectories=False)\n"
     ]
    }
   ],
   "source": [
    "print(high_conf_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Detection: {\n",
      "    'id': '62a505c279f516bc805a24b1',\n",
      "    'attributes': BaseDict({}),\n",
      "    'tags': BaseList([]),\n",
      "    'label': 'tv',\n",
      "    'bounding_box': BaseList([\n",
      "        0.009705719351768494,\n",
      "        0.38865435962945644,\n",
      "        0.23089201450347902,\n",
      "        0.22951249673332966,\n",
      "    ]),\n",
      "    'mask': None,\n",
      "    'confidence': 0.9027981162071228,\n",
      "    'index': None,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "sample = high_conf_view.first()\n",
    "print(sample.ensemble.detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = high_conf_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████| 4985/4985 [5.7m elapsed, 0s remaining, 14.3 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 4985/4985 [3.0m elapsed, 0s remaining, 26.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "eval_key=\"eval_retinanet_r_50_fpn_1x\"\n",
    "\n",
    "results = high_conf_view.evaluate_detections(\n",
    "    load_model_name,\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=eval_key,\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"key\": \"eval_retinanet_r_50_fpn_1x\",\n",
      "    \"version\": \"0.16.2\",\n",
      "    \"timestamp\": \"2022-06-11T21:17:10.859000\",\n",
      "    \"config\": {\n",
      "        \"method\": \"coco\",\n",
      "        \"cls\": \"fiftyone.utils.eval.coco.COCOEvaluationConfig\",\n",
      "        \"pred_field\": \"ensemble\",\n",
      "        \"gt_field\": \"ground_truth\",\n",
      "        \"iou\": 0.5,\n",
      "        \"classwise\": true,\n",
      "        \"iscrowd\": \"iscrowd\",\n",
      "        \"use_masks\": false,\n",
      "        \"use_boxes\": false,\n",
      "        \"tolerance\": null,\n",
      "        \"compute_mAP\": true,\n",
      "        \"iou_threshs\": [\n",
      "            0.5,\n",
      "            0.55,\n",
      "            0.6,\n",
      "            0.65,\n",
      "            0.7,\n",
      "            0.75,\n",
      "            0.8,\n",
      "            0.85,\n",
      "            0.9,\n",
      "            0.95\n",
      "        ],\n",
      "        \"max_preds\": 100,\n",
      "        \"error_level\": 1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.get_evaluation_info(eval_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        person       0.51      0.92      0.66     17204\n",
      "           car       0.30      0.84      0.45      2384\n",
      "         chair       0.22      0.70      0.33      2035\n",
      "          book       0.39      0.85      0.54      2749\n",
      "        bottle       0.30      0.80      0.43      1413\n",
      "           cup       0.27      0.77      0.40       940\n",
      "  dining table       0.12      0.66      0.21       708\n",
      " traffic light       0.18      0.69      0.28       656\n",
      "          bowl       0.21      0.77      0.33       656\n",
      "       handbag       0.13      0.59      0.21       540\n",
      "          bird       0.39      0.83      0.53       832\n",
      "          boat       0.27      0.80      0.40       599\n",
      "         truck       0.18      0.80      0.30       419\n",
      "         bench       0.14      0.64      0.23       501\n",
      "      umbrella       0.33      0.81      0.47       499\n",
      "           cow       0.46      0.87      0.60       541\n",
      "        banana       0.38      0.81      0.51       678\n",
      "      backpack       0.12      0.58      0.19       371\n",
      "        carrot       0.26      0.80      0.39       607\n",
      "    motorcycle       0.34      0.85      0.49       447\n",
      "         sheep       0.38      0.89      0.53       515\n",
      "  potted plant       0.18      0.74      0.29       345\n",
      "    wine glass       0.28      0.75      0.41       387\n",
      "         donut       0.55      0.85      0.66       571\n",
      "          kite       0.47      0.86      0.61       549\n",
      "         knife       0.14      0.59      0.22       334\n",
      "      broccoli       0.22      0.81      0.35       418\n",
      "          cake       0.31      0.84      0.46       455\n",
      "       bicycle       0.20      0.72      0.31       323\n",
      "      suitcase       0.32      0.82      0.46       397\n",
      "            tv       0.32      0.81      0.46       288\n",
      "        orange       0.22      0.69      0.33       328\n",
      "         pizza       0.27      0.78      0.40       287\n",
      "           bus       0.37      0.85      0.52       288\n",
      "        remote       0.22      0.68      0.33       283\n",
      "          vase       0.19      0.73      0.30       279\n",
      "         horse       0.30      0.87      0.44       273\n",
      "     surfboard       0.23      0.69      0.34       269\n",
      "         zebra       0.59      0.91      0.72       292\n",
      "         clock       0.31      0.79      0.45       267\n",
      "   sports ball       0.45      0.83      0.58       454\n",
      "    cell phone       0.18      0.69      0.28       262\n",
      "         couch       0.19      0.74      0.31       261\n",
      "      elephant       0.53      0.94      0.68       313\n",
      "           tie       0.26      0.72      0.38       296\n",
      "         spoon       0.11      0.58      0.19       253\n",
      "          skis       0.15      0.71      0.24       241\n",
      "         apple       0.23      0.70      0.35       348\n",
      "       giraffe       0.55      0.91      0.69       232\n",
      "        laptop       0.28      0.79      0.41       231\n",
      " tennis racket       0.39      0.85      0.53       225\n",
      "          sink       0.18      0.79      0.29       225\n",
      "           dog       0.33      0.82      0.47       218\n",
      "          fork       0.17      0.69      0.27       215\n",
      "           cat       0.42      0.90      0.57       202\n",
      "    teddy bear       0.28      0.73      0.40       191\n",
      "         train       0.36      0.86      0.50       190\n",
      "    skateboard       0.32      0.83      0.46       179\n",
      "        toilet       0.39      0.87      0.54       179\n",
      "      sandwich       0.18      0.66      0.28       177\n",
      "           bed       0.18      0.67      0.29       163\n",
      "      keyboard       0.25      0.84      0.38       153\n",
      "baseball glove       0.28      0.66      0.39       148\n",
      "  baseball bat       0.21      0.60      0.31       146\n",
      "      airplane       0.38      0.89      0.53       143\n",
      "          oven       0.18      0.72      0.29       143\n",
      "       hot dog       0.29      0.69      0.40       163\n",
      "  refrigerator       0.20      0.76      0.31       126\n",
      "       frisbee       0.46      0.89      0.61       115\n",
      "         mouse       0.34      0.84      0.48       106\n",
      "  fire hydrant       0.45      0.85      0.59       101\n",
      "     stop sign       0.32      0.80      0.46        75\n",
      "          bear       0.56      0.92      0.69        71\n",
      "     snowboard       0.13      0.68      0.22        69\n",
      " parking meter       0.29      0.75      0.41        60\n",
      "    toothbrush       0.09      0.54      0.16        57\n",
      "     microwave       0.29      0.85      0.43        55\n",
      "      scissors       0.08      0.44      0.14        36\n",
      "    hair drier       0.04      0.27      0.07        11\n",
      "       toaster       0.13      0.78      0.23         9\n",
      "\n",
      "     micro avg       0.32      0.83      0.47     48769\n",
      "     macro avg       0.28      0.76      0.40     48769\n",
      "  weighted avg       0.37      0.83      0.50     48769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the 10 most common classes in the dataset\n",
    "counts = dataset.count_values(\"ground_truth.detections.label\")\n",
    "classes_top = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Print a classification report for the top-10 classes\n",
    "results.print_report(classes=classes_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3931411489089599\n"
     ]
    }
   ],
   "source": [
    "print(results.mAP())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59b3c8b4d184c9fa97cb69be64af4ac62d59b4aa4e9f32c8b33c27b6076bd8d0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('objectdetection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
